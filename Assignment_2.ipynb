{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngzhankang/Deep-Learning_ca2/blob/main/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BYpyxj0Fk5Y"
      },
      "source": [
        "# Assignment 2 - Part A\r\n",
        "Done by : \r\n",
        "- P1935727 Ng Zhan Kang\r\n",
        "- P1935488 Triston Loh\r\n",
        "\r\n",
        "Class of DIT/FT/2B/11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jw-5Jb2IFo8z"
      },
      "source": [
        "---\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfuLF6F5Fqfy"
      },
      "source": [
        "### 1.1 Ensuring 0% Util\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "Ensure that our slot give by Google is not utilized yet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BGRQF99FXN0",
        "outputId": "530efacd-fd40-4ba7-ae0c-71f62fe91f55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# to ensure that the current gpu utilization is 0\r\n",
        "# memory footprint support libraries/code\r\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\r\n",
        "!pip install gputil\r\n",
        "!pip install psutil\r\n",
        "!pip install humanize\r\n",
        "import psutil\r\n",
        "import humanize\r\n",
        "\r\n",
        "import os\r\n",
        "import GPUtil as GPU\r\n",
        "GPUs = GPU.getGPUs()\r\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\r\n",
        "gpu = GPUs[0]\r\n",
        "def printm():\r\n",
        " process = psutil.Process(os.getpid())\r\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\r\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\r\n",
        "printm() "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7411 sha256=e932030e95b9319e6a565349c98874de0830e4be0bdda3563878c346c0837b74\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.8 GB  | Proc size: 111.9 MB\n",
            "GPU RAM Free: 15079MB | Used: 0MB | Util   0% | Total 15079MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqrHcr-0FsgA"
      },
      "source": [
        "### 1.2. Forcing Utils To 0% To Get A Clean Cluster\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "Please do not use this step unless the cluster you are allocated to is quite full."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTOEqthPFtzb"
      },
      "source": [
        "## if utilization is > 0, run this code(keep running this cell and the above cell till the util number is 0%):\r\n",
        "## NOTE THAT RUNNING THIS MIGHT KILL GPU SESSION AND RESULT IN DATA LOSS(NOT ADVICABLE TO KEEP ON REUSING)\r\n",
        "# !kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzwbFfRDFvLA"
      },
      "source": [
        "---\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5Xuk11lFxdF"
      },
      "source": [
        "# 2.Ensuring GPU Is Utilized In Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUlSke_yFy_u"
      },
      "source": [
        "### 2.1. See the list of available devices\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "This entire section can be omitted if users are not utilizing GPU at all."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ayxTCVYFvrc",
        "outputId": "97353154-d1f3-47ea-c778-8799346caccf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\r\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 9380353951587859317\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14638920512\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 16954954492919506826\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HMHDnUiF046",
        "outputId": "d05fef67-f3d8-4ef1-8534-7ba6f825e4ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\r\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyQoebJvF2QE",
        "outputId": "68ce20e2-4cbd-44d7-b969-43de790b457d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Dec 20 12:13:53 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P0    30W /  70W |    227MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSKJeK-TF0ju"
      },
      "source": [
        "---\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0S6R2BRF8CT"
      },
      "source": [
        "# 3.Background Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXUod3TUEj28"
      },
      "source": [
        "### 3.1.About The CelebA Dataset\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVghCYGCE7zV"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzqp36BfEz01"
      },
      "source": [
        "### 3.2.CelebA Dataset\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwrnNBzfE7Lq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHHorNnYE9Ji"
      },
      "source": [
        "---\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goBQn4VDE-2j"
      },
      "source": [
        "# 4.Data Importing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZbRGCO7FEBV"
      },
      "source": [
        "### 4.1.Load the libraries\r\n",
        "---\r\n",
        "Load the necessary libraries for usage in the entire project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUaPaw8bE9oU"
      },
      "source": [
        "# Suppress Future Warnings\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8hXdBCFF_aK",
        "outputId": "dba1d7c3-bf8a-4518-c18f-7fe4c3bc9d54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# check versions of libraries we are going to use\r\n",
        "%tensorflow_version 2.x\r\n",
        "import os\r\n",
        "import tensorflow\r\n",
        "import sklearn\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib\r\n",
        "import platform\r\n",
        "\r\n",
        "message=\"        Versions        \"\r\n",
        "print(\"*\"*len(message))\r\n",
        "print(message)\r\n",
        "print(\"*\"*len(message))\r\n",
        "print(\"Tensorflow version={}\".format(tensorflow.__version__))\r\n",
        "print(\"Keras version={}\".format(tensorflow.keras.__version__))\r\n",
        "print(\"Sklearn version={}\".format(sklearn.__version__))\r\n",
        "print(\"Numpy version={}\".format(np.__version__))\r\n",
        "print(\"Pandas version={}\".format(pd.__version__))\r\n",
        "print(\"Seaborn version={}\".format(sns.__version__))\r\n",
        "print(\"Matplotlib version={}\".format(matplotlib.__version__))\r\n",
        "print(\"Python version={}\".format(platform.python_version()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "************************\n",
            "        Versions        \n",
            "************************\n",
            "Tensorflow version=2.4.0\n",
            "Keras version=2.4.0\n",
            "Sklearn version=0.22.2.post1\n",
            "Numpy version=1.19.4\n",
            "Pandas version=1.1.5\n",
            "Seaborn version=0.11.0\n",
            "Matplotlib version=3.2.2\n",
            "Python version=3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ed0ti4zF_xT"
      },
      "source": [
        "# download the necessary libraries that is not inside keras library\r\n",
        "\r\n",
        "# start importing necessary libraries\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.models import Sequential, load_model\r\n",
        "from tensorflow.keras.layers import Dense, Reshape, Dropout   \r\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, BatchNormalization\r\n",
        "from tensorflow.keras.layers import LeakyReLU\r\n",
        "from tensorflow.keras.layers import AveragePooling2D, MaxPooling2D\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import numpy as np\r\n",
        "import time\r\n",
        "\r\n",
        "# for making graphcial progress bar\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "# for adding support to different image file type\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orLlQsWzSREG"
      },
      "source": [
        "url = ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIBkRmL-KeTC"
      },
      "source": [
        "### 4.2.Creating `hms_string` class for time recording purposes\r\n",
        "---\r\n",
        "We create `hms_string` class for time recording purposes later on when we train our models, so that we can see the time elapsed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2dRdVM_KTWD"
      },
      "source": [
        "# Nicely formatted time string\r\n",
        "def hms_string(sec_elapsed):\r\n",
        "    h = int(sec_elapsed / (60 * 60))\r\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\r\n",
        "    s = sec_elapsed % 60\r\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}